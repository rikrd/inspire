{
 "metadata": {
  "name": "",
  "signature": "sha256:03e1867cc48d267a000dde868c35c1a89e110d21fcab4d1e139b4dae49656828"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Getting started\n",
      "## Importing the necessary modules\n",
      "To start with we will import the modules we are going to use throughout this tutorial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import inspire"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The dataset\n",
      "Next we download the dataset, load it, and have a quick peak of what is in it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset_filename = inspire.download_dataset()\n",
      "dataset = inspire.load_dataset(dataset_filename)\n",
      "\n",
      "print('There are {} tokens in dataset'.format(len(dataset['tokens'])))\n",
      "\n",
      "inspire.pprint(dataset['tokens']['35541'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 3239 tokens in dataset\n",
        "{u'noise_onset': 532600,\n",
        " u'noise_transcription': u'BAB4.txt',\n",
        " u'noise_type': u'bab4',\n",
        " u'noise_wav': u'BAB4.wav',\n",
        " u'signal_wav': u'T_35541.wav',\n",
        " u'snr': -2.039,\n",
        " u'speaker': u's1',\n",
        " u'speech': u'pequen\u0303os'}\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our data is a class deriving from a Python dictionary containing an entry named `tokens` with a mapping from a stimulus ID to a stimulus (also named token) presented to a set of listeners.\n",
      "The stimulus itself is represented as a dictionary, with entries for several properties of the stimuli presented: \n",
      "\n",
      "* **type of noise** `noise_type`\n",
      "* **speaker ID** `speaker`\n",
      "* **signal-to-noise ratio** `snr`\n",
      "\n",
      "The filenames of the audios:\n",
      "\n",
      "* **signal wave file** `signal_wav`\n",
      "* **noise wave file** `noise_wav`\n",
      "\n",
      "And the underlying word that was presented as well as those that were reported by the listeners:\n",
      "\n",
      "* **presented word** `speech`\n",
      "\n",
      "## The lexicon\n",
      "Since all the tasks are based on the pronunciations of the words, our next step is to load a lexicon.\n",
      "The lexicon is not in JSON format, it is in a forma that HTK understands, therefore parsing it takes a bit more work. Luckily for you we have a written a function to do just that:\n",
      "\n",
      "We will first download the lexicon, load it, and have a quick peak of what is in it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lexicon_filename = inspire.download_lexicon()\n",
      "lexicon = inspire.load_lexicon(lexicon_filename)\n",
      "\n",
      "print('There are {} words in the lexicon'.format(len(lexicon)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 738967 words in the lexicon\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lexicon is a subclass of a Python dictionary mapping from a word to a set of pronunciations.  The pronunciations are lists of strings where each string represents a phoneme.\n",
      "\n",
      "To use the lexicon we simply query it as we would do with any other Python dictionary.  Note that the case of the words is ignored when querying the lexicon:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inspire.pprint('Is \"ayuda\" in the lexicon? {}'.format('ayuda' in lexicon))\n",
      "inspire.pprint(lexicon['ayuda'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'Is \"ayuda\" in the lexicon? True'\n",
        "[[u'a', u'j', u'j', u'\u02c8u', u'\u00f0', u'a']]\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The submission\n",
      "\n",
      "### Preparation of the submission\n",
      "A submission consists of a dictionary data structure containing metadata (information of the participant, the challenge edition and contact details) and the predictions for the stimuli.  The predictions are stored in a data structure similar to that of the dataset (a list of dictionaries, one per stimulus).  Each stimulus dictionary must contain the `token_id` and then a list of task predictions `task_prediction` each task prediction is a dictionary containing a `task` property indicating what task it is solving and the `prediction` which is dictionary whose form depends on the task.\n",
      "\n",
      "In order to avoid errors when creating the participation data structure, we have built a Python class that eases the job."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "submission = inspire.Submission(authors='Ricard Marxer', email='r.marxer@sheffield.ac.uk', challenge_edition='0.1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Adding the predictions of the tasks\n",
      "We iterate over the dataset and for each presented word we query it's pronunciations, and arbitrarily select one.\n",
      "We will then produce the predictions for each of the tasks.  In this simple baseline random guessing scenario we will follow the same predcition strategy independently of the token:\n",
      "\n",
      "* **where task** For the phoneme positions we predict a 50% chance of observing a confusion. We predict a 1% chance of confusion at the inter-phoneme positions and a 30% chance of confusion at the positions before and after the utterance.\n",
      "* **where task** We only provide predictions for the phoneme positions. We predict a 50% chance of not observing a confusion (the observed phoneme will remain) and a 5% of removing the phoneme.\n",
      "* **full task** We predict a 10% of not observing a confusion (the ellicited pronunciation being reported)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Iterate over all the stimuli in our dataset\n",
      "for token_id, token  in dataset['tokens'].items():\n",
      "    word = token['speech']\n",
      "    \n",
      "    # Lexicon may contain multiple pronunciations\n",
      "    pronunciations = lexicon[word.upper()]\n",
      "    \n",
      "    # Here we arbitrarily select the first one\n",
      "    pronunciation = pronunciations[0]\n",
      "    \n",
      "    # Get a list of phonemes from the space separated pronunciation of the lexicon\n",
      "    phonemes = pronunciation\n",
      "    \n",
      "    # Possible indices of confusions\n",
      "    # are the number of phonemes plus\n",
      "    # the number of positions around phonemes\n",
      "    index_count = len(phonemes)*2 + 1\n",
      "    \n",
      "    confusion_probabilities = np.zeros(index_count)\n",
      "\n",
      "    # Our random guess for the Where task assumes:\n",
      "    #  - a 30% chance of finding a confusion at all phoneme positions (substitutions or removals)\n",
      "    confusion_probabilities[1::2] = 0.3\n",
      "    \n",
      "    #  - a 1% chance of finding an insertion between all phoneme positions (insertions)\n",
      "    confusion_probabilities[2:-2:2] = 0.01\n",
      "    \n",
      "    #  - a 50% chance of finding insertions of phonemes at beginning and end of words (insertions)\n",
      "    confusion_probabilities[0] = 0.5\n",
      "    confusion_probabilities[-1] = 0.5\n",
      "            \n",
      "    submission.where_task(token_id, confusion_probabilities)\n",
      "    \n",
      "    # Our random guess for the What task assumes:\n",
      "    # Phonemes are represented as strings\n",
      "    # Sequence of phonemes are space joined strings\n",
      "    # A removal is represented as an empty string\n",
      "    for phoneme_index, phoneme in enumerate(phonemes):\n",
      "        index = phoneme_index * 2 + 1\n",
      "        \n",
      "        #  - a 50% chance of not changing the phoneme\n",
      "        submission.what_task(token_id, index, phoneme, 0.5)\n",
      "\n",
      "        #  - a 5% chance of phoneme removal\n",
      "        submission.what_task(token_id, index, '', 0.05)\n",
      "        \n",
      "    # Our random guess for the Full task assumes:\n",
      "    #  - a 10% chance of reporting the pronunciation of the presented utterance\n",
      "    submission.full_task(token_id, ' '.join(pronunciation), 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the submission has been created we can print the predictions of a given token.\n",
      "This is also useful to understand the JSON format of a submission."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inspire.pprint(submission['tokens']['35541'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'full': {u'pronunciations_probability': {u'p e k \u02c8e \u0272 o s': 0.1}},\n",
        " u'what': {u'per_index_phonemes_probability': {'1': {'': 0.05, u'p': 0.5},\n",
        "                                               '11': {'': 0.05, u'o': 0.5},\n",
        "                                               '13': {'': 0.05, u's': 0.5},\n",
        "                                               '3': {'': 0.05, u'e': 0.5},\n",
        "                                               '5': {'': 0.05, u'k': 0.5},\n",
        "                                               '7': {'': 0.05,\n",
        "                                                     u'\u02c8e': 0.5},\n",
        "                                               '9': {'': 0.05,\n",
        "                                                     u'\u0272': 0.5}}},\n",
        " u'where': {u'confusion_probability': [0.5,\n",
        "                                       0.29999999999999999,\n",
        "                                       0.01,\n",
        "                                       0.29999999999999999,\n",
        "                                       0.01,\n",
        "                                       0.29999999999999999,\n",
        "                                       0.01,\n",
        "                                       0.29999999999999999,\n",
        "                                       0.01,\n",
        "                                       0.29999999999999999,\n",
        "                                       0.01,\n",
        "                                       0.29999999999999999,\n",
        "                                       0.01,\n",
        "                                       0.29999999999999999,\n",
        "                                       0.5]}}\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also save the submission as a JSON file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "submission.save('submission_random_guess.json')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = submission.submit('test123')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'evaluation': {}}\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
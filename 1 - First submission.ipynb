{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# First submission: A random guess\n",
      "## Installing the necessary module\n",
      "\n",
      "You must first install the `inspirespeech` Python module.\n",
      "\n",
      "This can be done using `pip` by running the following command in your terminal:\n",
      "```\n",
      "pip install inspirespeech\n",
      "```\n",
      "\n",
      "You may also run the following command to upgrade to the latest version:\n",
      "```\n",
      "pip install --upgrade inspirespeech\n",
      "```\n",
      "\n",
      "After installing or upgrading the `inspirespeech` module one must **restart the IPython Notebook**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Importing the necessary modules\n",
      "To start with we will import the modules we are going to use throughout this tutorial."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import numpy as np\n",
      "import inspirespeech as inspire"
     ],
     "language": "python"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The setting\n",
      "An evaluation setting is composed of a dataset, a lexicon and several evaluation parameters. A submission will always be tied to a given evaluation setting.  Therefore our first job is to get an evaluation setting to work on.\n",
      "\n",
      "By not passing any parameters we will be getting the default one.  Currently the default setting is a dataset of words in Spanish and a large vocabulary lexicon in IPA encoding."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "setting = inspire.get_evaluation_setting()"
     ],
     "language": "python",
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The dataset\n",
      "Next we download the dataset, load it, and have a quick peek at what is in it."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "There are 3235 tokens in dataset\n"
       ]
      }
     ],
     "input": [
      "dataset_filename = setting.download_dataset()\n",
      "dataset = inspire.load_dataset(dataset_filename)\n",
      "\n",
      "print('There are {} tokens in dataset'.format(len(dataset['tokens'])))"
     ],
     "language": "python",
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We may now print a token of the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "{u'noise_onset': 532600,\n",
        " u'noise_transcription': u'BAB4.txt',\n",
        " u'noise_type': u'bab4',\n",
        " u'noise_wav': u'BAB4.wav',\n",
        " u'signal_wav': u'T_35541.wav',\n",
        " u'snr': -2.039,\n",
        " u'speaker': u's1',\n",
        " u'speech': u'pequeños'}\n"
       ]
      }
     ],
     "input": [
      "inspire.pprint(dataset['tokens']['35541'])"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our data is a class deriving from a Python dictionary containing an entry named `tokens` with a mapping from a stimulus ID to a stimulus (also named token) presented to a set of listeners.\n",
      "The stimulus itself is represented as a dictionary, with entries for several properties of the stimulus presented: \n",
      "\n",
      "* **type of noise** `noise_type`\n",
      "* **speaker ID** `speaker`\n",
      "* **signal-to-noise ratio** `snr`\n",
      "\n",
      "The filenames of the audios:\n",
      "\n",
      "* **signal wave file** `signal_wav`\n",
      "* **noise wave file** `noise_wav`\n",
      "\n",
      "And the underlying word that was presented as well as those that were reported by the listeners:\n",
      "\n",
      "* **presented word** `speech`\n",
      "\n",
      "Additionally there are 10% of the tokens in the dataset that are considered tokens belonging to the development set and for which the responses given by listeners are provided.  These may be used for training purposes."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "{u'noise_onset': 790069,\n",
        " u'noise_transcription': u'BAB8.txt',\n",
        " u'noise_type': u'bab8',\n",
        " u'noise_wav': u'BAB8.wav',\n",
        " u'responses': {u'manda': 2,\n",
        "                u'mandas': 2,\n",
        "                u'mando': 1,\n",
        "                u'mandos': 1,\n",
        "                u'mangos': 1,\n",
        "                u'mano': 1,\n",
        "                u'manos': 6,\n",
        "                u'mantas': 1},\n",
        " u'signal_wav': u'T_36504.wav',\n",
        " u'snr': 0.4,\n",
        " u'speaker': u's1',\n",
        " u'speech': u'mandan'}\n"
       ]
      }
     ],
     "input": [
      "inspire.pprint(dataset['tokens']['36504'])"
     ],
     "language": "python",
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The lexicon\n",
      "Since all the tasks are based on the pronunciations of the words, our next step is to load a lexicon.\n",
      "The lexicon is not in JSON format, it is in a format that HTK understands, therefore parsing it takes a bit more work. Luckily for you we have a written a function to do just that:\n",
      "\n",
      "We will first download the lexicon, load it, and have a quick peek at what is in it."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "There are 885434 words in the lexicon\n"
       ]
      }
     ],
     "input": [
      "lexicon_filename = setting.download_lexicon()\n",
      "lexicon = inspire.load_lexicon(lexicon_filename)\n",
      "\n",
      "print('There are {} words in the lexicon'.format(len(lexicon)))"
     ],
     "language": "python",
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lexicon is a subclass of a Python dictionary mapping from a word to a set of pronunciations.  The pronunciations are lists of strings where each string represents a phoneme.\n",
      "\n",
      "To use the lexicon we simply query it as we would do with any other Python dictionary.  Note that the case of the words is ignored when querying the lexicon:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "'Is \"ayuda\" in the lexicon? True'\n",
        "[[u'a', u'jj', u'ˈu', u'ð', u'a']]\n"
       ]
      }
     ],
     "input": [
      "inspire.pprint('Is \"ayuda\" in the lexicon? {}'.format('ayuda' in lexicon))\n",
      "inspire.pprint(lexicon['ayuda'])"
     ],
     "language": "python",
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The alignments\n",
      "Using the pronunciations in the lexicon, we can print all the alignments accepted for a pair of words: "
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "\u001b[31mm\u001b[m \u001b[31mˌa\u001b[m \u001b[31ml\u001b[m \u001b[31me\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "\u001b[41m \u001b[m \u001b[41m  \u001b[m \u001b[31mɛ\u001b[m \u001b[31mn\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "---\n",
        "\u001b[31mm\u001b[m \u001b[31mˌa\u001b[m \u001b[31ml\u001b[m \u001b[31me\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "\u001b[41m \u001b[m \u001b[31mɛ \u001b[m \u001b[41m \u001b[m \u001b[31mn\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "---\n",
        "\u001b[31mm\u001b[m \u001b[31mˌa\u001b[m \u001b[31ml\u001b[m \u001b[31me\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "\u001b[31mɛ\u001b[m \u001b[41m  \u001b[m \u001b[41m \u001b[m \u001b[31mn\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "---\n",
        "\u001b[31mm\u001b[m \u001b[31mˌa\u001b[m \u001b[31ml\u001b[m \u001b[31me\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "\u001b[41m \u001b[m \u001b[31mɛ \u001b[m \u001b[31mn\u001b[m \u001b[41m \u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "---\n",
        "\u001b[31mm\u001b[m \u001b[31mˌa\u001b[m \u001b[31ml\u001b[m \u001b[31me\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "\u001b[31mɛ\u001b[m \u001b[41m  \u001b[m \u001b[31mn\u001b[m \u001b[41m \u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "---\n",
        "\u001b[31mm\u001b[m \u001b[31mˌa\u001b[m \u001b[31ml\u001b[m \u001b[31me\u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "\u001b[31mɛ\u001b[m \u001b[31mn \u001b[m \u001b[41m \u001b[m \u001b[41m \u001b[m \u001b[32mt\u001b[m \u001b[32mˈɛ\u001b[m \u001b[32mɾ\u001b[m \u001b[32mɔ\u001b[m \n",
        "---\n"
       ]
      }
     ],
     "input": [
      "edit_scripts = inspire.get_edit_scripts(lexicon['maletero'][0], lexicon['entero'][0])\n",
      "for edit_script in edit_scripts:\n",
      "    inspire.print_edit_script(edit_script)\n",
      "    print('---')"
     ],
     "language": "python",
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The submission\n",
      "To make submissions to the INSPIRE Challenge you must have an account on the INSPIRE Challenge: [Click here to register](http://143.167.9.43:5000/register)\n",
      "\n",
      "Note don't use a secure password, since in order to simplify the tasks it may be sent as plain text to the server.\n",
      "The email and password will be used in the following steps to create and submit our predictions.\n",
      "\n",
      "### Preparation of the submission\n",
      "A submission consists of a dictionary data structure containing metadata (information of the participant, the challenge edition and contact details) and the predictions for the stimuli.  The predictions are stored in a data structure similar to that of the dataset (a list of dictionaries, one per stimulus).  Each stimulus dictionary must contain the `token_id` and then a list of task predictions `task_prediction` each task prediction is a dictionary containing a `task` property indicating what task it is solving and the `prediction` which is dictionary whose form depends on the task.\n",
      "\n",
      "In order to avoid errors when creating the participation data structure, we have built a Python class that eases the job."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "submission = inspire.Submission(email='dummy@email.com',\n",
      "                                description='''Random Guess:\n",
      "where task: For the phoneme positions we predict a 50% chance of observing a confusion. We predict a 1% chance of confusion at the inter-phoneme positions and a 30% chance of confusion at the positions before and after the utterance.\n",
      "what task: We only provide predictions for the phoneme positions, we do not provide predictions for inter-phoneme positions. We predict a 50% chance of not observing a confusion (the presented phoneme will remain) and a 5% of deleting the phoneme.\n",
      "full task: We predict a 10% chance of not observing a confusion (the ellicited pronunciation being reported).''',\n",
      "                                evaluation_setting=setting)"
     ],
     "language": "python",
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Adding the predictions of the tasks\n",
      "We iterate over the dataset and for each presented word we query it's pronunciations, and arbitrarily select one.\n",
      "We will then produce the predictions for each of the tasks.  In this simple baseline random guessing scenario we will follow the same predcition strategy independently of the token:\n",
      "\n",
      "* **where task** For the phoneme positions we predict a 50% chance of observing a confusion. We predict a 1% chance of confusion at the inter-phoneme positions and a 30% chance of confusion at the positions before and after the utterance.\n",
      "* **what task** We only provide predictions for the phoneme positions, we do not provide predictions for inter-phoneme positions. We predict a 50% chance of not observing a confusion (the presented phoneme will remain) and a 5% of deleting the phoneme.\n",
      "* **full task** We predict a 10% chance of not observing a confusion (the ellicited pronunciation being reported)."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# Iterate over all the stimuli in our dataset\n",
      "for token_id, token  in dataset['tokens'].items():   \n",
      "    # Lexicon may contain multiple pronunciations (we arbitrarily select the first)\n",
      "    word = token['speech']\n",
      "    pronunciation = lexicon[word][0]\n",
      "    \n",
      "    # Possible indices of confusions\n",
      "    # are the number of phonemes plus\n",
      "    # the number of positions around phonemes\n",
      "    index_count = len(pronunciation)*2 + 1\n",
      "    \n",
      "    confusion_probabilities = np.zeros(index_count)\n",
      "\n",
      "    # Our random guess for the Where task assumes:\n",
      "    #  - a 30% chance of finding a confusion at all phoneme positions (substitutions or removals)\n",
      "    confusion_probabilities[1::2] = 0.3\n",
      "    \n",
      "    #  - a 1% chance of finding an insertion between all phoneme positions (insertions)\n",
      "    confusion_probabilities[2:-2:2] = 0.01\n",
      "    \n",
      "    #  - a 50% chance of finding insertions of phonemes at beginning and end of words (insertions)\n",
      "    confusion_probabilities[0] = 0.5\n",
      "    confusion_probabilities[-1] = 0.5\n",
      "            \n",
      "    submission.where_task(token_id, confusion_probabilities)\n",
      "    \n",
      "    # Our random guess for the What task assumes:\n",
      "    # Phonemes are represented as strings\n",
      "    # Sequence of phonemes are space joined strings\n",
      "    # A removal is represented as an empty string\n",
      "    for phoneme_index, phoneme in enumerate(pronunciation):\n",
      "        index = phoneme_index*2 + 1\n",
      "        \n",
      "        non_confusion_probability = 1 - confusion_probabilities[phoneme_index]\n",
      "        \n",
      "        #  - a naive guess (same as used in the where task) of not changing the phoneme\n",
      "        submission.what_task(token_id, index, phoneme, non_confusion_probability)\n",
      "\n",
      "        #  - a 5% chance of phoneme removal\n",
      "        submission.what_task(token_id, index, '', 0.05)\n",
      "        \n",
      "    # Our random guess for the Full task assumes:\n",
      "    #  - a 10% chance of reporting the pronunciation of the presented utterance\n",
      "    submission.full_task(token_id, pronunciation, 0.1)"
     ],
     "language": "python",
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the submission has been created we can print the predictions of a given token.\n",
      "This is also useful to understand the JSON format of a submission."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "{u'full': {u'p e k ˈe ɲ o s': 0.1},\n",
        " u'what': {'1': {'': 0.05, u'p': 0.5},\n",
        "           '11': {'': 0.05, u'o': 0.69999999999999996},\n",
        "           '13': {'': 0.05, u's': 0.98999999999999999},\n",
        "           '3': {'': 0.05, u'e': 0.69999999999999996},\n",
        "           '5': {'': 0.05, u'k': 0.98999999999999999},\n",
        "           '7': {'': 0.05, u'ˈe': 0.69999999999999996},\n",
        "           '9': {'': 0.05, u'ɲ': 0.98999999999999999}},\n",
        " u'where': [0.5,\n",
        "            0.29999999999999999,\n",
        "            0.01,\n",
        "            0.29999999999999999,\n",
        "            0.01,\n",
        "            0.29999999999999999,\n",
        "            0.01,\n",
        "            0.29999999999999999,\n",
        "            0.01,\n",
        "            0.29999999999999999,\n",
        "            0.01,\n",
        "            0.29999999999999999,\n",
        "            0.01,\n",
        "            0.29999999999999999,\n",
        "            0.5]}\n"
       ]
      }
     ],
     "input": [
      "inspire.pprint(submission['tokens']['35541'])"
     ],
     "language": "python",
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also save the submission as a JSON file."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "submission.save('submission_random_guess.gz')"
     ],
     "language": "python",
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluating before submitting\n",
      "\n",
      "Sometimes we might not want to submit, and simply evaluate a submission.  In that case we may call `evaluate()`.\n",
      "\n",
      "The evaluation is only performed on the development subset to allow a fast evaluation."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "job_eval = submission.evaluate(password='dummypassword')"
     ],
     "language": "python",
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `job` that we obtain is an object that allows us to check if the result is available using `job.ready()`, wait for it to finish using `job.wait()` and obtain the result using `job.result()`.\n",
      "\n",
      "**Note: ** Calling `job.wait()` will block until the result is ready or the job has failed."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "Is the job ready? False\n"
       ]
      }
     ],
     "input": [
      "print('Is the job ready? {}'.format(job_eval.ready()))"
     ],
     "language": "python",
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "job_eval.wait()"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "-4.373493237234398\n",
        "-40.502463124955675\n",
        "-56.1343281231216\n"
       ]
      }
     ],
     "input": [
      "result = job_eval.result()\n",
      "\n",
      "inspire.pprint(result['where']['token_averaged'])\n",
      "inspire.pprint(result['what']['token_averaged'])\n",
      "inspire.pprint(result['full']['token_averaged'])"
     ],
     "language": "python",
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sending in the submission\n",
      "\n",
      "The final step is to send the submission. This will save a copy of the submission in the server and return the evaluation results.\n",
      "\n",
      "The submission **may take some time**, especially if the submission covers all tasks and all tokens.\n",
      "\n",
      "To submit we simply call the submit function of the submission object that we have created.\n",
      "The email that we have used when creating the submission class is the one that will be used to login to the server.\n",
      "We must also supply the password that we used when we registered to the INSPIRE Challenge website."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "job_sub = submission.submit(password='dummypassword')"
     ],
     "language": "python",
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "job_sub.wait()"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "-4.313680440028883\n",
        "-40.381556409910374\n",
        "-55.761984968206306\n"
       ]
      }
     ],
     "input": [
      "result = job_sub.result()\n",
      "inspire.pprint(result['where']['token_averaged'])\n",
      "inspire.pprint(result['what']['token_averaged'])\n",
      "inspire.pprint(result['full']['token_averaged'])"
     ],
     "language": "python",
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In some cases it may happen that the evaluation for some of the tokens could not be conducted. If this is the case the result will have an **```error```** key in it. Most common error is a wrong number of elements for the **```where```** or **```what```** tasks.  This may happen when using a pronunciation different than the one in the lexicon provided by the evaluation setting. This would lead to a different number of indices and probabilities being provided and therefore the likelihood fo the observations cannot be computed.\n",
      "\n",
      "In our case there are no erronous tokens."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "prompt_number": 20,
       "metadata": {},
       "output_type": "pyout",
       "text": [
        "[]"
       ]
      }
     ],
     "input": [
      "filter(lambda x: 'error' in x[1], result['where']['tokens'].items())"
     ],
     "language": "python",
     "prompt_number": 20
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {
  "name": "",
  "signature": "sha256:da5203f3e7954799fe5cf5d99ab833c98f10e30c2e976749d21412c8fd675519"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
